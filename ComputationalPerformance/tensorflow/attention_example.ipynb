{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "document = 'Review on the global epidemiological situation and the efficacy of chloroquine ' \\\n",
    "           'and hydroxychloroquine for the treatment of COVID-19. ' \\\n",
    "           'Covid-19 disease is caused by SARS-CoV-2, a virus belonging to the coronavirus family. ' \\\n",
    "           'Covid-19 is so new that there is currently no specific vaccine or treatment. ' \\\n",
    "           'Clinical trials are currently underway. In vitro tests are also being conducted to assess the efficacy of ' \\\n",
    "           'chloroquine and hydroxychloroquine for the treatment of this epidemic, ' \\\n",
    "           'which is considered a pandemic by the WHO. We note that the content of this review is dated. ' \\\n",
    "           'The information it contains is subject to change and modification as the epidemic progresses.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carico il modello per la lingua inglese di Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1102 08:53:29.929197 78996 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1102 08:53:31.307605 78996 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "english_model = spacy.load('en')\n",
    "\n",
    "spacy_doc = english_model(document)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Spacy prima di tutto divide il documento in singoli \"token\" (parole, punteggiatura, numeri ecc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Review, on, the, global, epidemiological, situation, and, the, efficacy, of, chloroquine, and, hydroxychloroquine, for, the, treatment, of, COVID-19, ., Covid-19, disease, is, caused, by, SARS, -, CoV-2, ,, a, virus, belonging, to, the, coronavirus, family, ., Covid-19, is, so, new, that, there, is, currently, no, specific, vaccine, or, treatment, ., Clinical, trials, are, currently, underway, ., In, vitro, tests, are, also, being, conducted, to, assess, the, efficacy, of, chloroquine, and, hydroxychloroquine, for, the, treatment, of, this, epidemic, ,, which, is, considered, a, pandemic, by, the, WHO, ., We, note, that, the, content, of, this, review, is, dated, ., The, information, it, contains, is, subject, to, change, and, modification, as, the, epidemic, progresses, .]\n"
     ]
    }
   ],
   "source": [
    "print(list(spacy_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assegna a ogni token un codice (Part-of-Speeach Tag) per identificare il suo ruolo nella frase, come nell'analisi grammaticale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review NOUN\n",
      "on ADP\n",
      "the DET\n",
      "global ADJ\n",
      "epidemiological ADJ\n",
      "situation NOUN\n",
      "and CCONJ\n",
      "the DET\n",
      "efficacy NOUN\n",
      "of ADP\n",
      "chloroquine NOUN\n",
      "and CCONJ\n",
      "hydroxychloroquine NOUN\n",
      "for ADP\n",
      "the DET\n",
      "treatment NOUN\n",
      "of ADP\n",
      "COVID-19 NOUN\n",
      ". PUNCT\n",
      "Covid-19 ADJ\n",
      "disease NOUN\n",
      "is AUX\n",
      "caused VERB\n",
      "by ADP\n",
      "SARS PROPN\n",
      "- PUNCT\n",
      "CoV-2 PROPN\n",
      ", PUNCT\n",
      "a DET\n",
      "virus NOUN\n",
      "belonging VERB\n",
      "to ADP\n",
      "the DET\n",
      "coronavirus PROPN\n",
      "family NOUN\n",
      ". PUNCT\n",
      "Covid-19 PROPN\n",
      "is AUX\n",
      "so ADV\n",
      "new ADJ\n",
      "that SCONJ\n",
      "there PRON\n",
      "is AUX\n",
      "currently ADV\n",
      "no DET\n",
      "specific ADJ\n",
      "vaccine NOUN\n",
      "or CCONJ\n",
      "treatment NOUN\n",
      ". PUNCT\n",
      "Clinical ADJ\n",
      "trials NOUN\n",
      "are AUX\n",
      "currently ADV\n",
      "underway ADJ\n",
      ". PUNCT\n",
      "In ADP\n",
      "vitro X\n",
      "tests NOUN\n",
      "are AUX\n",
      "also ADV\n",
      "being AUX\n",
      "conducted VERB\n",
      "to PART\n",
      "assess VERB\n",
      "the DET\n",
      "efficacy NOUN\n",
      "of ADP\n",
      "chloroquine NOUN\n",
      "and CCONJ\n",
      "hydroxychloroquine NOUN\n",
      "for ADP\n",
      "the DET\n",
      "treatment NOUN\n",
      "of ADP\n",
      "this DET\n",
      "epidemic NOUN\n",
      ", PUNCT\n",
      "which DET\n",
      "is AUX\n",
      "considered VERB\n",
      "a DET\n",
      "pandemic NOUN\n",
      "by ADP\n",
      "the DET\n",
      "WHO PROPN\n",
      ". PUNCT\n",
      "We PRON\n",
      "note VERB\n",
      "that SCONJ\n",
      "the DET\n",
      "content NOUN\n",
      "of ADP\n",
      "this DET\n",
      "review NOUN\n",
      "is AUX\n",
      "dated VERB\n",
      ". PUNCT\n",
      "The DET\n",
      "information NOUN\n",
      "it PRON\n",
      "contains VERB\n",
      "is AUX\n",
      "subject ADJ\n",
      "to ADP\n",
      "change NOUN\n",
      "and CCONJ\n",
      "modification NOUN\n",
      "as SCONJ\n",
      "the DET\n",
      "epidemic NOUN\n",
      "progresses VERB\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for t in spacy_doc:\n",
    "    print(t.text, t.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ad ogni parola possiamo associare il relativo word embedding. Per cui carico il modello di word embedding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "I1102 08:53:32.507402 78996 textcleaner.py:37] 'pattern' package not found; tag filters are not available for English\n",
      "I1102 08:53:32.512377 78996 utils.py:418] loading Word2Vec object from pub_med_retrained_ddi_word_embedding_200.model\n",
      "I1102 08:53:32.581598 78996 utils.py:452] loading wv recursively from pub_med_retrained_ddi_word_embedding_200.model.wv.* with mmap=None\n",
      "I1102 08:53:32.582594 78996 utils.py:487] setting ignored attribute vectors_norm to None\n",
      "I1102 08:53:32.583622 78996 utils.py:452] loading vocabulary recursively from pub_med_retrained_ddi_word_embedding_200.model.vocabulary.* with mmap=None\n",
      "I1102 08:53:32.584592 78996 utils.py:452] loading trainables recursively from pub_med_retrained_ddi_word_embedding_200.model.trainables.* with mmap=None\n",
      "I1102 08:53:32.584592 78996 utils.py:487] setting ignored attribute cum_table to None\n",
      "I1102 08:53:32.585588 78996 utils.py:424] loaded pub_med_retrained_ddi_word_embedding_200.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word_model = Word2Vec.load('pub_med_retrained_ddi_word_embedding_200.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E creo una matrice di word vectors, cio√® l'input della rete ricorrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = np.zeros(shape=(1, 300, 200))\n",
    "for i in range(len(spacy_doc)):\n",
    "    t = spacy_doc[i]\n",
    "    lower_text = t.text.lower()\n",
    "    if lower_text in word_model.wv.vocab:\n",
    "        word_matrix[0][i] = word_model.wv.get_vector(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisco la rete ricorrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300, 160)          179840    \n",
      "_________________________________________________________________\n",
      "attention_weights (Attention (None, 160)               460       \n",
      "=================================================================\n",
      "Total params: 180,300\n",
      "Trainable params: 180,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Bidirectional, Dense\n",
    "from attention_extraction_layers import AttentionWeights, ContextVector\n",
    "from keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(300, 200))\n",
    "lstm_layer = Bidirectional(LSTM(80, return_sequences=True))(input_layer)\n",
    "attention_weights = AttentionWeights(300, name='attention_weights')(lstm_layer)\n",
    "context_vector = ContextVector()([lstm_layer, attention_weights])\n",
    "dense = Dense(4, activation='softmax')(context_vector)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=attention_weights)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In teoria dovrei allenarla con migliaia di documenti... ma ci metteremmo ore.\n",
    "Supponendo che sia stata allenata, estraiamo i pesi dell'attention.\n",
    "Per fare questo io devo \"tagliare il modello dell'ultimo pezzo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruisco quindi un modello \"intermedio\", con lo stesso input di quello precedente e gli stessi livelli tranne ContextVector e Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer_model = Model(inputs=model.input, outputs=[model.get_layer('attention_weights').output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo modello, passo il mio documento di prova:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "doc_attention = intermediate_layer_model.predict(word_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(doc_attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}