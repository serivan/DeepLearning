{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYouMsbZYWtC"
      },
      "source": [
        "# Introduzione a PyTorch\n",
        "\n",
        "Author: Nicola Arici (nicola.arici@unibs.it)\n",
        "\n",
        "## **What is PyTorch?**\n",
        "\n",
        "PyTorch is an open-source deep learning framework developed by Facebook’s AI Research team (FAIR). It's particularly popular for its flexibility and usability in research and production alike. Unlike TensorFlow's older versions, which used static computation graphs, PyTorch uses dynamic computation graphs, making it easier to debug and more intuitive for Python programmers.\n",
        "\n",
        "In this notebook, we will explore PyTorch's key features, followed by comparisons to Keras and TensorFlow.\n",
        "\n",
        "**Resources**\n",
        "\n",
        "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
        "- [Keras Documentation](https://keras.io/)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Installing PyTorch**\n",
        "\n",
        "Since we're on Colab, we have nothing to do.\n",
        "But if you are interested in running it locally, you can follow the instructions from [PyTorch's official website](https://pytorch.org/get-started/locally/) to choose the correct version for CPU or GPU.\n",
        "\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision torchaudio\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6i3DSLlYWtF"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Tensors: The Building Block of PyTorch**\n",
        "\n",
        "In PyTorch, tensors are the fundamental data structure, analogous to arrays in NumPy but with the added advantage that they can run on GPUs. In this section, we’ll explore various ways to create tensors and some basic operations that can be performed on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrK8gQRhYWtF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "DATA = [[1.0, 2.0], [3.0, 4.0]]\n",
        "\n",
        "np_array = np.array(DATA)\n",
        "print(f\"NumPy array: \\n {np_array}\")\n",
        "\n",
        "tensor_from_list =\n",
        "print(f\"\\nTensor from list:\\n {tensor_from_list}\")\n",
        "\n",
        "tensor_from_numpy =\n",
        "print(f\"\\nTensor from NumPy array:\\n {tensor_from_numpy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TKtCp0DYWtH"
      },
      "source": [
        "**Creating Tensors with Special Initialization**\n",
        "\n",
        "PyTorch provides several functions to create tensors with specific initial values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6SRsAmKYWtH"
      },
      "outputs": [],
      "source": [
        "# Creating a tensor of zeros\n",
        "zeros_tensor =\n",
        "print(f\"Tensor of zeros:\\n {zeros_tensor}\")\n",
        "\n",
        "# Creating a tensor of ones\n",
        "ones_tensor =\n",
        "print(f\"\\nTensor of ones:\\n {ones_tensor}\")\n",
        "\n",
        "# Creating a tensor with random values\n",
        "random_tensor =\n",
        "print(f\"\\nRandom tensor:\\n {random_tensor}\")\n",
        "\n",
        "rand_like_tensor =\n",
        "print(f\"\\nRandom tensor with the same shape as the previous tensor:\\n {rand_like_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QbFM2MJYWtH"
      },
      "source": [
        "**Moving Tensors Between Devices (CPU and GPU)**\n",
        "\n",
        "One of the key advantages of PyTorch is its seamless support for GPU acceleration. PyTorch allows tensors to be created on or moved between devices like CPUs and GPUs. This is done using the ```torch.device()``` object and the ```to()``` method. If a GPU is available, computations *can* be much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9VBic4iYWtH"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "# Check if GPU is available\n",
        "device =\n",
        "\n",
        "tensor_on_gpu = torch.rand((2, 3), )\n",
        "print(\"Tensor on GPU (if available):\\n\", tensor_on_gpu)\n",
        "\n",
        "\n",
        "# Moving a tensor from CPU to GPU\n",
        "tensor_cpu = torch.ones((2, 3))\n",
        "\n",
        "s = time()\n",
        "result = tensor_cpu ** 2 * tensor_cpu ** 5\n",
        "print(f\"\\nTime taken on CPU: {round((time() - s)*1000, 6)} ms\")\n",
        "\n",
        "tensor_gpu =\n",
        "s = time()\n",
        "result = tensor_gpu ** 2 * tensor_gpu ** 5\n",
        "print(f\"Time taken on GPU: {round((time() - s)*1000, 6)} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsDyu4MnYWtI"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Automatic Differentiation: PyTorch’s Autograd**\n",
        "\n",
        "In deep learning, we often need to calculate gradients during backpropagation to update the weights of a neural network. [PyTorch’s autograd module](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) is responsible for automatically computing the gradients of tensors during the backward pass. It does this by building a [dynamic computational graph](https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/), where nodes represent operations and edges represent the flow of data.\n",
        "\n",
        "PyTorch tracks every operation on tensors with ```requires_grad=True``` to enable automatic differentiation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGH4Y6gcYWtI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([2.0, 3.0], )\n",
        "\n",
        "y = x[0] ** 3 + x[1] * 2\n",
        "\n",
        "print(f\"Results: {y} \\n\")\n",
        "\n",
        "print(f\"Gradients of x: {x.}\")\n",
        "print(f\"Backward Function of y: {y.}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS6_AGkMYWtI"
      },
      "outputs": [],
      "source": [
        "# Create a tensor with requires_grad=True\n",
        "a = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Perform operations with gradient tracking\n",
        "s = time()\n",
        "b = a ** 2\n",
        "print(f\"With gradient tracking {round((time() - s)*1000, 6)} ms\")\n",
        "\n",
        "# Disable gradient tracking\n",
        "with :\n",
        "    s = time()\n",
        "    c = a ** 2\n",
        "    print(f\"Without gradient tracking {round((time() - s)*1000, 6)} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMGHFkTEYWtJ"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **Building a Simple Neural Network in PyTorch**\n",
        "\n",
        "In this section, we will walk through the process of creating a simple neural network using PyTorch. All the components needed to build the network are contained in the [torch.nn](https://pytorch.org/docs/stable/nn.html) package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh4MrFqiYWtJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "INPUT = np.array([[1.0, 2.0]])\n",
        "\n",
        "# Define a simple network using Keras\n",
        "model = Sequential([\n",
        "    Input((2,)),\n",
        "    Dense(4, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "s = time()\n",
        "print(f\"\\nForward Pass: {model.predict(INPUT)} in {round((time() - s)*1000, 6)} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xUz27aIYWtJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "INPUT = torch.tensor([[1.0, 2.0]])\n",
        "\n",
        "net =\n",
        "\n",
        "print(net)\n",
        "\n",
        "s = time()\n",
        "print(f\"\\nForward Pass: {net(INPUT)} in {round((time() - s)*1000, 6)} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-8IhCJ3YWtJ"
      },
      "source": [
        "PyTorch neural networks are typically defined by subclassing torch.nn.Module, which represents a base class for all neural networks in PyTorch. Layers are defined in the ```__init__()``` method, and the forward pass is implemented in the ```forward()``` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpeq742CYWtJ"
      },
      "outputs": [],
      "source": [
        "class SimpleNet(nn.Module):\n",
        "\n",
        "\n",
        "net = SimpleNet()\n",
        "\n",
        "print(net)\n",
        "\n",
        "s = time()\n",
        "print(f\"\\nForward Pass: {net(INPUT)} in {round((time() - s)*1000, 6)} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7l_qqUeYWtJ"
      },
      "source": [
        "**Training a Neural Network in PyTorch**\n",
        "\n",
        "PyTorch is a powerful deep learning library that gives us a high degree of manual control over every step of the training process. Unlike Keras, which abstracts many of the internal workings behind easy-to-use functions, PyTorch allows us to customize every part of the model’s behavior. This can be especially useful when we need to fine-tune specific aspects of the training or modify the underlying logic to fit complex or non-standard tasks.\n",
        "\n",
        "We will begin by setting up a basic neural network model, define the [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions), and then proceed with training the network on a dataset. As we go, we'll manually implement essential components such as forward passes, backpropagation, and weight updates with [optimizer](https://pytorch.org/docs/stable/optim.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDXSCb-AYWtJ"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "\n",
        "net = SimpleNet()\n",
        "\n",
        "criterion =\n",
        "optimizer =\n",
        "\n",
        "# Train the network to understand if the input is positive or negative\n",
        "\n",
        "train_dataset = [\n",
        "    (torch.tensor([1.0, 2.0]), torch.tensor([1.0])),\n",
        "    (torch.tensor([-3.0, -4.0]), torch.tensor([0.0])),\n",
        "    (torch.tensor([5.0, 6.0]), torch.tensor([1.0])),\n",
        "    (torch.tensor([-5.0, -6.0]), torch.tensor([0.0])),\n",
        "]\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for tensor, target in train_dataset:\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {round(loss.item(), 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW0fP10EYWtK"
      },
      "source": [
        "## **PyTorch vs. TensorFlow/Keras**\n",
        "\n",
        "\n",
        "| Feature               | PyTorch                                    | TensorFlow/Keras                          |\n",
        "|-----------------------|--------------------------------------------|-------------------------------------------|\n",
        "| **API Level**          | Low-level, very flexible                   | High-level (Keras) or low-level (TF core) |\n",
        "| **Computation Graph**  | Dynamic (eager execution)                  | Dynamic (with TensorFlow 2.x)             |\n",
        "| **Ease of Use**        | More manual, but powerful                  | Keras is very user-friendly               |\n",
        "| **Community**          | Growing rapidly, dominant in research      | Strong support, widely adopted in industry|\n",
        "| **Ecosystem**          | Fewer add-ons (though fast-growing)        | Large ecosystem (e.g., TensorFlow Hub)    |\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "- PyTorch provides exceptional flexibility, making it a favorite for researchers.\n",
        "- TensorFlow has a mature ecosystem, but PyTorch’s dynamic nature is great for debugging and custom models.\n",
        "- Keras is best for beginners or when rapid prototyping is necessary.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}