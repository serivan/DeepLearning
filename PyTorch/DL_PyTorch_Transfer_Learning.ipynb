{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu42d1PUu_BS"
      },
      "source": [
        "# Transfer Learning\n",
        "Author: Nicola Arici (nicola.arici@unibs.it)\n",
        "\n",
        "Transfer learning is a key technique in deep learning that allows us to leverage the knowledge learned by a model trained on one task and apply it to another, often related, task. Instead of training a model from scratch—which can be computationally expensive and data-hungry—we start with a pretrained model and adapt it to our specific dataset. This approach is especially useful when working with limited data or resources.\n",
        "\n",
        "In this notebook, we will apply transfer learning using PyTorch. We’ll start with a ResNet50 model pretrained on the large-scale ImageNet dataset and adapt it to classify images from the Flower102 dataset.\n",
        "\n",
        "**Resources**\n",
        "\n",
        "- [Torchvision Models](https://docs.pytorch.org/vision/stable/models.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_6azO736FMa"
      },
      "source": [
        "First, we’ll explore the available pretrained models and learn how to load and use them in our workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyP4OzysuLKP"
      },
      "outputs": [],
      "source": [
        "from torchvision.models\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2J16Kdp6yZ7"
      },
      "source": [
        "Then, we’ll test the adapted model using a few new sample images to evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB6AhYtd2Pwu"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "\n",
        "TEST_IMAGE = {\n",
        "    'banana': 'https://cdn.gvmnet.it/admingvm/media/immagininews/fegatostomacoeintestino/banane_benefici.jpeg',\n",
        "    'space shuttle': 'https://media-cldnry.s-nbcnews.com/image/upload/t_fit-1500w,f_auto,q_auto:best/msnbc/Components/Photos/060519/060519_shuttle_vlg6p.jpg',\n",
        "    'French horn': 'https://www.normans.co.uk/cdn/shop/articles/french_horn.jpg?v=1663242562',\n",
        "    'beacon': 'https://aa-images.co.uk/wp-content/uploads/2021/11/157-New-Brighton-Lighthouse-on-the-beach-scaled-1.jpg',\n",
        "    'tiger': 'https://files.worldwildlife.org/wwfcmsprod/images/Tiger_resting_Bandhavgarh_National_Park_India/hero_small/6aofsvaglm_Medium_WW226365.jpg'\n",
        "}\n",
        "\n",
        "\n",
        "batch = None\n",
        "labels = []\n",
        "for label, img in TEST_IMAGE.items():\n",
        "    response = requests.get(img)\n",
        "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "\n",
        "    if batch is None:\n",
        "        batch = pImg.unsqueeze(0)   # (1, 3, 224, 224)\n",
        "    else:\n",
        "        batch = torch.cat((batch, pImg.unsqueeze(0)), dim=0)  # stack along batch dimension\n",
        "\n",
        "    labels.append(label)\n",
        "\n",
        "\n",
        "\n",
        "correct = 0\n",
        "for true_label, p in zip(labels, preds):\n",
        "    pred_label = p.argmax(dim=-1).item()\n",
        "\n",
        "    if true_label == class_names[pred_label]:\n",
        "        correct += 1\n",
        "\n",
        "print(f\"Accuracy: {correct}/{len(TEST_IMAGE)} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAAL6ibrB66d"
      },
      "source": [
        "The Flower102 dataset is a well-known benchmark for image classification tasks. It contains 8,189 images of flowers spanning 102 categories, including a wide variety of species commonly found in the United Kingdom. Each class has between 40 and 258 images, providing a moderate challenge due to its fine-grained distinctions and varying lighting conditions, scales, and poses.\n",
        "\n",
        "This dataset is often used to evaluate transfer learning and fine-tuning techniques, as it is small enough for rapid experimentation yet diverse enough to test a model’s ability to generalize across visually similar classes.\n",
        "\n",
        "**Resources**:\n",
        "\n",
        "- [Torchvision FLower102](https://docs.pytorch.org/vision/main/generated/torchvision.datasets.Flowers102.html#torchvision.datasets.Flowers102)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrqVAh8M8est"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import Flowers102\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_set = Flowers102(root='data', split='train', download=True, transform=)\n",
        "test_set = Flowers102(root='data', split='test', download=True, transform=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iB4YvLg7VL9"
      },
      "source": [
        "Some stats..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "folCpFG6DPZ9"
      },
      "outputs": [],
      "source": [
        "print(f\"# Trainset: {len(train_set)}\")\n",
        "print(f\"# Valset: {len(val_set)}\")\n",
        "print(f\"# Testsett: {len(test_set)}\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(f\"Data shape: {train_set[0][0].shape}\")\n",
        "print(f\"# classes: {len(train_set.classes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGf0ZQQSD2g5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "RESNET_MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
        "RESNET_STD = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "def denorm(img):\n",
        "    img = img.permute(1, 2, 0)  # CxHxW → HxWxC\n",
        "    img = img * RESNET_STD + RESNET_MEAN       # undo normalization\n",
        "    img = torch.clamp(img, 0, 1) # keep in [0,1]\n",
        "    return img.numpy()\n",
        "\n",
        "random.seed(34566754)\n",
        "images, labels = zip(*[train_set[i] for i in random.choices(range(len(train_set)), k=10)])\n",
        "\n",
        "# Create 5x5 grid\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = images[i]\n",
        "\n",
        "    ax.imshow(denorm(img))\n",
        "    ax.set_title(train_set.classes[labels[i]], fontsize=9, pad=8)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvGC9r2u7X52"
      },
      "source": [
        "And let's test ResNet50 over FLower102"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f9K-YB37brJ"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "headers = [\"True\", \"Predicted\"]\n",
        "data = []\n",
        "\n",
        "for image, label in zip(images, labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(image.unsqueeze(0).to(device))\n",
        "        pred_label = preds.argmax(dim=-1).item()\n",
        "\n",
        "        data.append([ train_set.classes[label], class_names[pred_label]])\n",
        "\n",
        "print(tabulate(data, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKXp2sz59K91"
      },
      "source": [
        "Let's create a custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rb3F1j-dUkh"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "class FlowerModel(nn.Module):\n",
        "\n",
        "\n",
        "\n",
        "    def train_classifier(self, train_dataloader, val_dataloader, epochs, optimizer, early_stop_ep=5):\n",
        "\n",
        "        train_loss = []\n",
        "        train_accs = []\n",
        "\n",
        "        val_loss = []\n",
        "        val_accs = []\n",
        "\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            self.train()\n",
        "            running_loss = 0.0\n",
        "            acc = 0\n",
        "            total = 0\n",
        "\n",
        "            for it, (images, labels) in enumerate(tqdm(train_dataloader, desc=\"Train\")):\n",
        "\n",
        "                images = images.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.forward(images)\n",
        "                loss = self.criterion(logits, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Accuracy\n",
        "                predictions = torch.max(logits, 1).indices\n",
        "\n",
        "                acc += (predictions.detach().cpu() == labels.detach().cpu()).sum().item()\n",
        "                total += len(labels)\n",
        "\n",
        "            epoch_loss = running_loss/len(train_dataloader)\n",
        "            train_loss.append(epoch_loss)\n",
        "\n",
        "            epoch_acc = acc*100/total\n",
        "            train_accs.append(epoch_acc)\n",
        "\n",
        "            ### VALIDATION\n",
        "            ep_val_loss, ep_val_acc = self.eval_classifier(val_dataloader)\n",
        "\n",
        "            val_loss.append(ep_val_loss)\n",
        "            val_accs.append(ep_val_acc)\n",
        "\n",
        "\n",
        "            print(f\"\\nEpoch {ep+1}: Val Loss {round(ep_val_loss, 3)} - Val Accuracy {round(ep_val_acc, 2)}%\\n\")\n",
        "\n",
        "\n",
        "        return train_loss, train_accs, val_loss, val_accs\n",
        "\n",
        "\n",
        "    def eval_classifier(self, dataloader):\n",
        "        self.eval()\n",
        "\n",
        "        val_loss = 0\n",
        "        val_accs = 0\n",
        "        total = 0\n",
        "\n",
        "        for it, (images, labels) in enumerate(tqdm(dataloader, desc=\"Val\")):\n",
        "\n",
        "            with torch.no_grad():\n",
        "                images = images.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.forward(images)\n",
        "\n",
        "                loss = self.criterion(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                predictions = torch.max(logits, 1).indices\n",
        "                val_accs += (predictions.detach().cpu() == labels.detach().cpu()).sum().item()\n",
        "                total += len(labels)\n",
        "\n",
        "        return val_loss/len(dataloader), val_accs/total*100\n",
        "\n",
        "\n",
        "    def predict(self, dataloader):\n",
        "        self.eval()\n",
        "\n",
        "        predictions = []\n",
        "        for it, (images, labels) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "            with torch.no_grad():\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                logits = self.forward(images)\n",
        "                predictions.extend(logits.detach().cpu().numpy().tolist())\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkiVLPV3c4hF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BS = 32\n",
        "\n",
        "trainloader = DataLoader(train_set, batch_size=BS, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_set, batch_size=BS, num_workers=2)\n",
        "testloader = DataLoader(test_set, batch_size=BS, num_workers=2)\n",
        "\n",
        "\n",
        "flowerModel = FlowerModel(device, freeze=True).to(device)\n",
        "optimizer =\n",
        "\n",
        "epochs = 8\n",
        "\n",
        "train_loss, train_accs, val_loss, val_accs = flowerModel.train_classifier(trainloader, valloader, epochs, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_S_Jdghigz7"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "axs[0].plot(range(len(train_loss)), train_loss, label=\"Train\")\n",
        "axs[0].plot(range(len(val_loss)), val_loss, label=\"Validation\")\n",
        "axs[0].set_xlabel(\"No. of Epoch\")\n",
        "axs[0].set_ylabel(\"Loss\")\n",
        "axs[0].set_title(\"Training Loss\")\n",
        "axs[0].grid()\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(range(len(train_accs)), train_accs, label=\"Train\")\n",
        "axs[1].plot(range(len(val_accs)), val_accs, label=\"Validation\")\n",
        "axs[1].set_xlabel(\"No. of Epoch\")\n",
        "axs[1].set_ylabel(\"Accuracy %\")\n",
        "axs[1].set_title(\"Training Accuracy\")\n",
        "axs[1].grid()\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dJEHVPCkU5s"
      },
      "outputs": [],
      "source": [
        "_, test_acc = flowerModel.eval_classifier(testloader)\n",
        "print(f\"\\nAccuracy on Testset: {round(test_acc, 2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jqi__61lXq-"
      },
      "outputs": [],
      "source": [
        "optimizer =\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "train_loss, train_accs, val_loss, val_accs = flowerModel.train_classifier(trainloader, valloader, epochs, optimizer)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "axs[0].plot(range(len(train_loss)), train_loss, label=\"Train\")\n",
        "axs[0].plot(range(len(val_loss)), val_loss, label=\"Validation\")\n",
        "axs[0].set_xlabel(\"No. of Epoch\")\n",
        "axs[0].set_ylabel(\"Loss\")\n",
        "axs[0].set_title(\"Training Loss\")\n",
        "axs[0].grid()\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(range(len(train_accs)), train_accs, label=\"Train\")\n",
        "axs[1].plot(range(len(val_accs)), val_accs, label=\"Validation\")\n",
        "axs[1].set_xlabel(\"No. of Epoch\")\n",
        "axs[1].set_ylabel(\"Accuracy %\")\n",
        "axs[1].set_title(\"Training Accuracy\")\n",
        "axs[1].grid()\n",
        "axs[1].legend()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CYboANymYj4"
      },
      "outputs": [],
      "source": [
        "_, test_acc = flowerModel.eval_classifier(testloader)\n",
        "print(f\"\\nAccuracy on Testset: {round(test_acc, 2)}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}