{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1113b437"
      },
      "source": [
        "# **Data Extraction and Cleaning**"
      ],
      "id": "1113b437"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.12.1\n",
        "!pip install torchdata==0.4.1"
      ],
      "metadata": {
        "id": "2y6CLi1su_Th"
      },
      "id": "2y6CLi1su_Th",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afa60efb"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import IMDB\n",
        "import pandas as pd\n",
        "\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for label, line in train_iter:\n",
        "    sentences.append(line)\n",
        "    labels.append(label)\n",
        "\n",
        "\n",
        "test_iter = IMDB(split='test')\n",
        "\n",
        "for label, line in test_iter:\n",
        "    sentences.append(line)\n",
        "    labels.append(label)\n",
        "\n",
        "df = pd.DataFrame({\"review\": sentences, \"sentiment\": [1 if each == \"pos\" else 0 for each in labels]})\n",
        "\n",
        "df"
      ],
      "id": "afa60efb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fd019cb"
      },
      "outputs": [],
      "source": [
        "# Descriptive statistics\n",
        "df.describe()"
      ],
      "id": "0fd019cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b612300e"
      },
      "outputs": [],
      "source": [
        "# Identifying missing values\n",
        "\n",
        "df.isnull().sum()"
      ],
      "id": "b612300e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1561acb"
      },
      "outputs": [],
      "source": [
        "# Duplicated review - Duplicated row\n",
        "# If this two values are equal that means the same review have the same labels\n",
        "\n",
        "print(df.review.duplicated().sum(), df.duplicated().sum())\n",
        "\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "id": "e1561acb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaZadR07e7AC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "id": "SaZadR07e7AC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3828a095"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')"
      ],
      "id": "3828a095"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e4f1ec2"
      },
      "outputs": [],
      "source": [
        "mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
        "           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
        "           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
        "           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
        "           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
        "           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
        "           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n",
        "           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
        "           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
        "           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
        "           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
        "           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
        "           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
        "           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
        "           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
        "           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n",
        "           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
        "           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
        "           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
        "           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n",
        "           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
        "           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
        "           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
        "           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
        "           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
        "           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
        "           \"you're\": \"you are\", \"you've\": \"you have\" }"
      ],
      "id": "0e4f1ec2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88c3a4d1"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Function to clean data\n",
        "\n",
        "def clean_text(text,lemmatize = True):\n",
        "    soup = BeautifulSoup(text, \"html.parser\") # Remove html tags\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Expanding chatwords and contracts clearing contractions\n",
        "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
        "    emoji_clean= re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    text = emoji_clean.sub(r'',text)\n",
        "    text = re.sub(r'\\.(?=\\S)', '. ',text)   # Add space after full stop\n",
        "    text = re.sub(r'http\\S+', '', text)     # Remove urls\n",
        "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) # Remove punctuation\n",
        "    \n",
        "    # Return token\n",
        "    return \" \".join([word for word in text.split() if word not in stop and word.isalpha()])"
      ],
      "id": "88c3a4d1"
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].apply(clean_text)"
      ],
      "metadata": {
        "id": "U9-SQTO7-fJ2"
      },
      "id": "U9-SQTO7-fJ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c55e339"
      },
      "source": [
        "# **Data analysis** "
      ],
      "id": "7c55e339"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0484394d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count Plot\n",
        "\n",
        "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
        "sns.countplot(x=df.sentiment, palette=['green','red'], order=[1, 0])\n",
        "plt.xticks(ticks=np.arange(2), labels=['positive','negative'])\n",
        "plt.title('Sentiment count for IMBD reviews')\n",
        "plt.show()"
      ],
      "id": "0484394d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dac0dedf"
      },
      "outputs": [],
      "source": [
        "print('Positive reviews are', (round(df['sentiment'].value_counts()[0])),'i.e.', round(df['sentiment'].value_counts()[0]/len(df)*100, 2), '% of the dataset')\n",
        "print('Negative reviews are', (round(df['sentiment'].value_counts()[1])),'i.e.',round(df['sentiment'].value_counts()[1]/len(df)*100, 2), '% of the dataset')"
      ],
      "id": "dac0dedf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99c06515"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Word cloud for positive reviews\n",
        "\n",
        "positive_data = df[df.sentiment == 1]['review']\n",
        "positive_data_string = ' '.join(positive_data)\n",
        "\n",
        "wc = WordCloud(max_words=2000, width=1200, height=600, background_color=\"white\").generate(positive_data_string)\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "id": "99c06515"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ec6faea"
      },
      "outputs": [],
      "source": [
        "# Word cloud for negative reviews\n",
        "\n",
        "negative_data = df[df.sentiment == 0]['review']\n",
        "negative_data_string = ' '.join(negative_data)\n",
        "\n",
        "wc = WordCloud(max_words=2000, width=1200, height=600, background_color=\"white\").generate(negative_data_string)\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "id": "7ec6faea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "016e345f"
      },
      "outputs": [],
      "source": [
        "text_len_pos = positive_data.str.len()\n",
        "text_len_neg = negative_data.str.len()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
        "ax1.hist(text_len_pos, color='green')\n",
        "ax1.set_title('Positive Reviews')\n",
        "ax1.set_xlabel('Number of Characters')\n",
        "ax1.set_ylabel('Count')\n",
        "\n",
        "ax2.hist(text_len_neg, color='red')\n",
        "ax2.set_title('Negative Reviews')\n",
        "ax2.set_xlabel('Number of Characters')\n",
        "ax2.set_ylabel('Count')\n",
        "fig.suptitle('Number of characters in texts')\n",
        "plt.show()"
      ],
      "id": "016e345f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75b06eb5"
      },
      "outputs": [],
      "source": [
        "text_len_pos = positive_data.str.split().map(lambda x: len(x))\n",
        "text_len_neg = negative_data.str.split().map(lambda x: len(x))\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,8))\n",
        "ax1.hist(text_len_pos, color='green')\n",
        "ax1.set_title('Positive Reviews')\n",
        "ax1.set_xlabel('Number of Words')\n",
        "ax1.set_ylabel('Count')\n",
        "\n",
        "ax2.hist(text_len_neg, color='red')\n",
        "ax2.set_title('Negative Reviews')\n",
        "ax2.set_xlabel('Number of Words')\n",
        "ax2.set_ylabel('Count')\n",
        "fig.suptitle('Number of words in texts')\n",
        "plt.show()"
      ],
      "id": "75b06eb5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad386c9b"
      },
      "outputs": [],
      "source": [
        "word_pos = positive_data.str.split().apply(lambda x : len(x))\n",
        "word_neg = negative_data.str.split().apply(lambda x :len(x) )\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "sns.histplot(word_pos, ax=ax1, color=\"green\", kde=True, stat=\"density\", linewidth=0)\n",
        "ax1.set_title('Positive Reviews')\n",
        "ax1.set_xlabel('Number of words per review')\n",
        "\n",
        "sns.histplot(word_neg, ax=ax2, color=\"red\", kde=True, stat=\"density\", linewidth=0)\n",
        "ax2.set_title('Negative Reviews')\n",
        "ax2.set_xlabel('Number of words per review')\n",
        "fig.suptitle('Distribution of number of words per reviews')\n",
        "plt.show()"
      ],
      "id": "ad386c9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41f49ae8"
      },
      "source": [
        "\n",
        "\n",
        "# **Predictive Modelling using LSTM** "
      ],
      "id": "41f49ae8"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "def prepare_sequence(rev, vocab, maxlen):\n",
        "    idxs = [vocab[w] if w in vocab else vocab[\"UNK\"] for w in rev.split()]\n",
        "    idxs = [idxs[i] if i < len(idxs) else vocab[\"PAD\"] for i in range(maxlen)]\n",
        "\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "class FilmReviewDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df, vocab, maxlen):\n",
        "        super(FilmReviewDataset, self).__init__()\n",
        "        \n",
        "        self.review = df.review.apply(prepare_sequence, args=(vocab, maxlen)).tolist()\n",
        "        self.sentiment = df.sentiment.tolist()\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rev = self.review[index]\n",
        "        sent = self.sentiment[index]\n",
        "        \n",
        "        return rev, sent\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.review)\n",
        "\n",
        "    def get_sentiment(self):\n",
        "        return self.sentiment\n"
      ],
      "metadata": {
        "id": "1-nU_QJJZlWg"
      },
      "id": "1-nU_QJJZlWg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting into train and test\n",
        "\n",
        "SEED = 11\n",
        "test_size = 0.2\n",
        "\n",
        "train, test = train_test_split(df, test_size=test_size, random_state=SEED)\n",
        "\n",
        "test, val = train_test_split(test, test_size=0.5, random_state=SEED)"
      ],
      "metadata": {
        "id": "hjvyqqm1bVZR"
      },
      "id": "hjvyqqm1bVZR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Vocabulary\n",
        "\n",
        "maxlen = 0\n",
        "vocab = {\"PAD\": 0, \"UNK\": 1}\n",
        "for rev in train[\"review\"]:\n",
        "    if len(rev.split()) > maxlen:\n",
        "        maxlen = len(rev.split())\n",
        "\n",
        "    for word in rev.split():\n",
        "        if word not in vocab:               # word has not been assigned an index yet\n",
        "            vocab[word] = len(vocab)        # Assign each word with a unique index\n",
        "\n",
        "print(maxlen, len(vocab))\n",
        "\n",
        "trainset = FilmReviewDataset(train, vocab, maxlen)\n",
        "valset = FilmReviewDataset(val, vocab,maxlen)\n",
        "testset = FilmReviewDataset(test, vocab, maxlen)"
      ],
      "metadata": {
        "id": "w1HUEkbTizBk"
      },
      "id": "w1HUEkbTizBk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentation used to built this model:\n",
        "\n",
        "\n",
        "1.   [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "2.   [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "3.   [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E9PtBsXc8O7Y"
      },
      "id": "E9PtBsXc8O7Y"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "class FilmModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, maxlen, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(FilmModel, self).__init__()\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        # From the nn.LSTM documentation: (L, N, D∗Hout), where:\n",
        "        L = maxlen          # L: sequence length\n",
        "                            # N: batch size\n",
        "        D = 2               # D: 2 if bidirectional 1 otherwise\n",
        "        Hout = hidden_dim   # Hout: hidden_dim\n",
        "\n",
        "        # The Classification Linear layer.\n",
        "        # So we have in input a matrix N x L*D*Hout with the same shape of the Linear weigth matrix\n",
        "        \n",
        "        self.cls = nn.Linear(L * D * Hout, 1)\n",
        "        \n",
        "\n",
        "    def forward(self, idxs):\n",
        "\n",
        "        # Compute embeddings\n",
        "        embeds = self.word_embeddings(idxs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "\n",
        "        return self.cls(lstm_out.view(len(idxs), -1))\n",
        "\n",
        "\n",
        "    def train_classifier(self, trainloader, valloader, epochs, criterion, optimizer, device):\n",
        "        train_losses = []\n",
        "        train_accs = []\n",
        "        val_losses = []\n",
        "        val_accs = []\n",
        "\n",
        "        best_epoch = 0\n",
        "        best_loss = np.Inf\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            self.train()\n",
        "            running_loss = 0.0\n",
        "            acc = 0\n",
        "            total = 0\n",
        "\n",
        "            for it, (revs, labels) in enumerate(tqdm(trainloader)):\n",
        "\n",
        "                revs = revs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.forward(revs)\n",
        "                loss = criterion(logits.squeeze(-1), labels.float())\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Accuracy\n",
        "                predictions = torch.round(torch.sigmoid(logits)).detach().cpu().numpy()\n",
        "                predictions = [int(p) for p in predictions]\n",
        "                acc += (predictions == labels.detach().cpu().numpy()).sum()\n",
        "\n",
        "\n",
        "                total += len(labels)\n",
        "            \n",
        "            train_loss = running_loss/len(trainloader)\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "            epoch_acc = acc*100/total\n",
        "            train_accs.append(epoch_acc)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = self.validation(valloader, criterion, device)\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            print(f\"\\nEpoch {ep+1}\")\n",
        "            if val_loss < best_loss:\n",
        "                print(f\"\\tBest validation loss improved from {round(best_loss, 3)} to {round(val_loss, 3)}\\n\")\n",
        "                torch.save(self.state_dict(), \"tuned_models.pt\")\n",
        "\n",
        "                best_loss = val_loss\n",
        "                best_epoch = ep + 1\n",
        "\n",
        "            print(f\"\\tTrain Loss {round(train_loss, 3)} - Train Accuracy {round(epoch_acc, 2)}%\")\n",
        "            print(f\"\\tValid Loss {round(val_loss, 3)} - Valid Accuracy {round(val_acc, 2)}%\\n\")\n",
        "\n",
        "        print(f\"Best model at epoch {best_epoch} saved in tuned_models.pt\")\n",
        "        return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "\n",
        "    def validation(self, dataloader, criterin, device):\n",
        "        self.eval()\n",
        "\n",
        "        running_loss = 0\n",
        "        acc = 0\n",
        "        total = 0\n",
        "\n",
        "        for it, (revs, labels) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "            with torch.no_grad():\n",
        "                revs = revs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                logits = self.forward(revs)\n",
        "                \n",
        "                loss = criterion(logits.squeeze(-1), labels.float())\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                predictions = torch.round(torch.sigmoid(logits)).detach().cpu().numpy()\n",
        "                predictions = [int(p) for p in predictions]\n",
        "                acc += (predictions == labels.detach().cpu().numpy()).sum()\n",
        "\n",
        "                total += len(labels)\n",
        "\n",
        "        val_loss = running_loss/len(dataloader)\n",
        "        val_acc = acc*100/total\n",
        "\n",
        "        return val_loss, val_acc\n",
        "\n",
        "\n",
        "    def predict(self, dataloader, device):\n",
        "        self.eval()\n",
        "\n",
        "        predictions = []\n",
        "        for it, (revs, labels) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "            with torch.no_grad():\n",
        "                revs = revs.to(device)\n",
        "                \n",
        "                logits = self.forward(revs)\n",
        "                \n",
        "                preds = torch.sigmoid(logits)\n",
        "                predictions.append(int(torch.round(preds).detach().cpu().numpy()))\n",
        "\n",
        "        \n",
        "        return predictions"
      ],
      "metadata": {
        "id": "mTwGDqiXbzQP"
      },
      "id": "mTwGDqiXbzQP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM  = 32\n",
        "LSTM_DIM = 64\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = FilmModel(maxlen, len(vocab), EMB_DIM, LSTM_DIM).to(device)"
      ],
      "metadata": {
        "id": "VqkrWb6ikYTA"
      },
      "id": "VqkrWb6ikYTA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n",
        "valloader = torch.utils.data.DataLoader(testset, batch_size=1)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1)\n",
        "\n",
        "epochs = 10\n",
        "train_losses, val_losses, train_accs, val_accs = model.train_classifier(trainloader, valloader, epochs, criterion, optimizer, device)"
      ],
      "metadata": {
        "id": "ONbMjipSkvZ-"
      },
      "id": "ONbMjipSkvZ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, epochs+1), train_losses, label=\"Training\")\n",
        "plt.plot(range(1, epochs+1), val_losses, label=\"Validation\")\n",
        "plt.xlabel(\"No. of Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g2mOUVuO1EEM"
      },
      "id": "g2mOUVuO1EEM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, epochs+1), train_accs, label=\"Training\")\n",
        "plt.plot(range(1, epochs+1), val_accs, label=\"Validation\")\n",
        "plt.xlabel(\"No. of Epoch\")\n",
        "plt.ylabel(\"Accuracy %\")\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XtvEdeMh1FAh"
      },
      "id": "XtvEdeMh1FAh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model = FilmModel(maxlen, len(vocab), EMB_DIM, LSTM_DIM)\n",
        "model.load_state_dict(torch.load(\"tuned_models.pt\"))\n",
        "model.to(device)\n",
        "\n",
        "predictions = model.predict(testloader, device)\n",
        "\n",
        "print()\n",
        "print(classification_report(testset.sentiment, predictions))"
      ],
      "metadata": {
        "id": "c16DZcL51tw5"
      },
      "id": "c16DZcL51tw5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2882.086048,
      "end_time": "2021-07-23T21:19:53.913137",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-23T20:31:51.827089",
      "version": "2.3.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}