{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Framework\n",
        "\n",
        "In this homework we will show the basic\n",
        "\n",
        "<br><br>\n",
        "*References*: <br>\n",
        "<a href=\"https://pytorch.org/docs/stable/\">PyTorch documentation</a> <br>\n"
      ],
      "metadata": {
        "id": "wrnqIPBufu1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the dataset** <br>\n",
        "\n",
        "*References*: <br>\n",
        "<a href=\"https://pytorch.org/vision/stable/index.html\">Torchvision</a><br>\n",
        "<a href=\"https://github.com/zalandoresearch/fashion-mnist\">FashionMNIST</a><br>\n",
        "<a href=\"https://pytorch.org/docs/stable/data.html\">DataLoader</a>"
      ],
      "metadata": {
        "id": "nZ-OMgJEf3by"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV_Xd29BfsJt",
        "outputId": "b9c576ff-9780-445e-b1ca-dc0bc3e130c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "\n",
        "train_set = FashionMNIST(\"./data\",\n",
        "                         download=True,\n",
        "                         train=True,\n",
        "                         transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "test_set = FashionMNIST(\"./data\",\n",
        "                        download=True,\n",
        "                        train=False,\n",
        "                        transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "train_set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def output_label(label):\n",
        "    output_mapping = {\n",
        "        0: \"T-shirt/Top\",\n",
        "        1: \"Trouser\",\n",
        "        2: \"Pullover\",\n",
        "        3: \"Dress\",\n",
        "        4: \"Coat\",\n",
        "        5: \"Sandal\",\n",
        "        6: \"Shirt\",\n",
        "        7: \"Sneaker\",\n",
        "        8: \"Bag\",\n",
        "        9: \"Ankle Boot\"\n",
        "    }\n",
        "    input = (label.item() if type(label) == torch.Tensor else label)\n",
        "    return output_mapping[input]\n",
        "\n",
        "\n",
        "a = next(iter(train_loader))\n",
        "print(\"Tensor size: (batch_size, n_channels, dim, dim)\")\n",
        "print(a[0].size())\n",
        "print()\n",
        "\n",
        "image, label = next(iter(train_set))\n",
        "\n",
        "plt.figure()\n",
        "plt.title(f\"Label: {output_label(label)}\")\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "Va6y59XHgZlR",
        "outputId": "9551903a-57d0-426d-f4ad-dae3e2376950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor size: (batch_size, n_channels, dim, dim)\n",
            "torch.Size([100, 1, 28, 28])\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrapolate the datasets dimensions.\n",
        "\n",
        "IMAGE_CHANNELS = image.size(0)\n",
        "IMAGE_DIM = image.size(1)\n",
        "NUM_LABELS = 10"
      ],
      "metadata": {
        "id": "WkWLbDc1F__d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feed Forward Model**\n",
        "\n",
        "*References*:<br>\n",
        "<a href=\"https://pytorch.org/docs/stable/nn.html\">torch.nn</a><br>\n",
        "<a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.view.html\">torch.view()</a>"
      ],
      "metadata": {
        "id": "-qWY4Tf8hkSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define custom Neural Network Module.\n",
        "# MANDATORY to define define the __init__ method (the Python custructor) and the forward() pass method.\n",
        "class FashionFF(nn.Module):\n",
        "\n",
        "    # Class constructor. Here we define the layers of our network.\n",
        "    def __init__(self, image_dim, image_channels, num_labels):\n",
        "        super(FashionFF, self).__init__()\n",
        "\n",
        "        # The input MUST be an array with size 784 (NOT a matrix 1x28x28)\n",
        "        self.linear1 = nn.Linear(image_channels*image_dim*image_dim, 16)\n",
        "        self.linear2 = nn.Linear(16, 40)\n",
        "        self.linear3 = nn.Linear(40, 80)\n",
        "        self.linear4 = nn.Linear(80, num_labels)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    # Forward pass. Here we define how the layers are concatenated.\n",
        "    # We can use all type of operation (complex pytorch functions like torch.cat or simple +,-,*,/ operations).\n",
        "    def forward(self, images):\n",
        "\n",
        "        # IMPORTANT: remember the size: (batch_size, num_channels, dim, dim)\n",
        "        # So we want to transform the 100 images 1x28x28 in 100 arrays with length 784.\n",
        "        # The view() function perform a reshape over the Tensor.\n",
        "        # With images.size(0), we tell the method to preserve the first dimension,\n",
        "        # and with -1 we tell the method to inferr the second dimension from the others remained in the input tensor (and so 1x28x28 = 784)\n",
        "        images = images.view(images.size(0), -1)\n",
        "\n",
        "        x = self.linear1(images)\n",
        "        x = self.linear2(self.dropout(x))\n",
        "        x = self.linear3(self.dropout(x))\n",
        "        x = self.linear4(self.dropout(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "IFNuKdqhhjTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, images_channels, num_labels):\n",
        "        super(FashionCNN, self).__init__()\n",
        "\n",
        "        # In the Sequential container we define a series of chained layer executed in order (like the Sequential keras block).\n",
        "        # In this case we regroup all the operatorions for a Convolutional Block.\n",
        "        # This is completely optional. You can stack layers upon layers as in Feed-Forward networks.\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=images_channels, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # The Sequential container is quick to code (no forward pass definition required) and the code is clearer to read, interpret and modify.\n",
        "        # But it has three major disadvantages:\n",
        "        # 1) You cannot edit the forward pass;\n",
        "        # 2) You cannot use useful function like \"view()\": you have to define a nn.Module that recreates the operation;\n",
        "        # 3) There is less control. For instance, you cannot debug inside the container (i.e. if you have incorrectly entered the size of a layer).\n",
        "\n",
        "        # As done for the FF network we flatten the matrix returned by the second Convolutional Block.\n",
        "        # How we derive 64x6x6 ?\n",
        "        # With the CNN theory of course, but a pratical way would be adding a print(out.shape) in our forward pass after the call to the second Sequential block.\n",
        "        self.fc1 = nn.Linear(in_features=64 * 6 * 6, out_features=600)\n",
        "\n",
        "        self.drop = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
        "        self.fc3 = nn.Linear(in_features=120, out_features=num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Tr-vay15iaGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Training Function**\n",
        "\n",
        "*We define only a trining loop for both networks.* <br>\n",
        "*This can be useful: less code to write.*"
      ],
      "metadata": {
        "id": "dyO46cvtEAtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, num_epochs, train_loader, test_loader, criterion, optimizer, scaler, device):\n",
        "\n",
        "    # Looping through epochs.\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "\n",
        "        # Set the model in \"train mode\" (i.e. dropout layer active)\n",
        "        model.train()\n",
        "\n",
        "        ep_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "\n",
        "            # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # To GPU (if available)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            ep_loss += loss.item()\n",
        "\n",
        "            # Backpropagation\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Parameters update.\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "\n",
        "        # Testing the model\n",
        "\n",
        "        # Set the model in \"test mode\" (i.e. dropout layer deactivated)\n",
        "        model.eval()\n",
        "\n",
        "        test_labels = []\n",
        "        predictions = []\n",
        "\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Disable gradient calculation to reduce memory consumption (we do not need gradient in prediction)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(images)\n",
        "\n",
        "            predictions += outputs.argmax(dim=-1).cpu().numpy().tolist()\n",
        "            test_labels += labels.cpu().numpy().tolist()\n",
        "\n",
        "        # Log Metrics.\n",
        "        print(f\"\\tEpoch: {epoch+1}, Loss: {round(ep_loss/len(train_loader), 4)}, Accuracy: {round(accuracy_score(test_labels, predictions)*100, 2)}%\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "d152GB43q1zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Hyperparameters and Training Tools** <br><br>\n",
        "\n",
        "*Reference*: <br>\n",
        "<a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">Loss Functions</a><br>\n",
        "<a href=\"https://pytorch.org/docs/stable/optim.html\">Optimizer</a><br>\n",
        "<a href=\"https://pytorch.org/docs/stable/amp.html#gradient-scaling\">Gradient Scaler</a>\n"
      ],
      "metadata": {
        "id": "Oou0hOcD4Szm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  torch.optim import AdamW\n",
        "\n",
        "# Define the torch device. REMEMBER to activate the GPU in the Runtime > Change type of Runtime settings.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model = FashionFF(IMAGE_DIM, IMAGE_CHANNELS, NUM_LABELS).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scaler = GradScaler()\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Lunch again train_model with tuned_ff to continue the training from the 11th epoch.\n",
        "tuned_ff = train_model(model, num_epochs, train_loader, test_loader, criterion, optimizer, scaler, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwQcotbwxPFu",
        "outputId": "c88eeb7c-2c45-430d-d0e1-d798141f4e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:18<02:44, 18.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 1, Loss: 0.8134, Accuracy: 80.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:27<01:41, 12.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 2, Loss: 0.6084, Accuracy: 81.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:35<01:15, 10.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 3, Loss: 0.5771, Accuracy: 82.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:43<00:58,  9.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 4, Loss: 0.5613, Accuracy: 82.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:52<00:47,  9.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 5, Loss: 0.5436, Accuracy: 83.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:01<00:36,  9.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 6, Loss: 0.5353, Accuracy: 83.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:09<00:26,  8.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 7, Loss: 0.5289, Accuracy: 83.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:17<00:17,  8.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 8, Loss: 0.5251, Accuracy: 83.4%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:26<00:08,  8.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 9, Loss: 0.5221, Accuracy: 83.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:34<00:00,  9.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 10, Loss: 0.516, Accuracy: 83.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Reusing the same code and changing only the model to be trained...*"
      ],
      "metadata": {
        "id": "mZ38cnMe4OU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FashionCNN(IMAGE_CHANNELS, NUM_LABELS).to(device)\n",
        "\n",
        "# Recreate the optimizer (with the new params) and the scaler rate (that was updated in the first training)\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "scaler = GradScaler()\n",
        "\n",
        "tuned_cnn = train_model(model, num_epochs, train_loader, test_loader, criterion, optimizer, scaler, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no5r9kSK0ttc",
        "outputId": "7ef3b879-a553-44dc-82eb-73e53eb0a051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:11<01:43, 11.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 1, Loss: 0.4288, Accuracy: 86.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:22<01:29, 11.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 2, Loss: 0.2919, Accuracy: 87.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:32<01:14, 10.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 3, Loss: 0.2544, Accuracy: 88.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:42<01:03, 10.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 4, Loss: 0.2322, Accuracy: 88.4%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:52<00:51, 10.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 5, Loss: 0.2124, Accuracy: 90.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:03<00:41, 10.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 6, Loss: 0.1957, Accuracy: 90.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:12<00:30, 10.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 7, Loss: 0.1802, Accuracy: 90.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:22<00:20, 10.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 8, Loss: 0.1697, Accuracy: 90.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:32<00:10, 10.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 9, Loss: 0.1574, Accuracy: 89.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:43<00:00, 10.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tEpoch: 10, Loss: 0.1503, Accuracy: 90.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}