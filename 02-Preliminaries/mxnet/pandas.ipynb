{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pandas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "Ya_BXN3r_UxV",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n",
        ":label:`sec_pandas`\n",
        "\n",
        "So far we have introduced a variety of techniques for manipulating data that are already stored in tensors.\n",
        "To apply deep learning to solving real-world problems,\n",
        "we often begin with preprocessing raw data, rather than those nicely prepared data in the tensor format.\n",
        "Among popular data analytic tools in Python, the `pandas` package is commonly used.\n",
        "Like many other extension packages in the vast ecosystem of Python,\n",
        "`pandas` can work together with tensors.\n",
        "So, we will briefly walk through steps for preprocessing raw data with `pandas`\n",
        "and converting them into the tensor format.\n",
        "We will cover more data preprocessing techniques in later chapters.\n",
        "\n",
        "## Reading the Dataset\n",
        "\n",
        "As an example, we begin by creating an artificial dataset that is stored in a\n",
        "csv (comma-separated values) file `../data/house_tiny.csv`. Data stored in other\n",
        "formats may be processed in similar ways.\n",
        "The following `mkdir_if_not_exist`\n",
        "function ensures that the directory `../data` exists.\n",
        "Note that the comment `#@save` is a special mark where the following function,\n",
        "class, or statements are saved in the `d2l` package\n",
        "so later they can be directly invoked (e.g., `d2l.mkdir_if_not_exist(path)`) without being redefined.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdhbAHlV_xWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet\n",
        "!pip install d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 1,
        "tab": [
          "mxnet"
        ],
        "id": "VEL2J7ho_Uxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def mkdir_if_not_exist(path):  #@save\n",
        "    \"\"\"Make a directory if it does not exist.\"\"\"\n",
        "    if not isinstance(path, str):\n",
        "        path = os.path.join(*path)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 2,
        "id": "YALR5wUJ_Uxv",
        "colab_type": "text"
      },
      "source": [
        "Below we write the dataset row by row into a csv file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "mxnet"
        ],
        "id": "C5U4Cfjx_Ux1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file = '../data/house_tiny.csv'\n",
        "mkdir_if_not_exist('../data')\n",
        "with open(data_file, 'w') as f:\n",
        "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
        "    f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
        "    f.write('2,NA,106000\\n')\n",
        "    f.write('4,NA,178100\\n')\n",
        "    f.write('NA,NA,140000\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "tFaa4puD_UyC",
        "colab_type": "text"
      },
      "source": [
        "To load the raw dataset from the created csv file,\n",
        "we import the `pandas` package and invoke the `read_csv` function.\n",
        "This dataset has four rows and three columns, where each row describes the number of rooms (\"NumRooms\"), the alley type (\"Alley\"), and the price (\"Price\") of a house.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 5,
        "tab": [
          "mxnet"
        ],
        "id": "RBXBKrUr_UyF",
        "colab_type": "code",
        "colab": {},
        "outputId": "f8e4b28a-f70f-4ebb-c13d-384c80f6f215"
      },
      "source": [
        "# If pandas is not installed, just uncomment the following line:\n",
        "# !pip install pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_file)\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   NumRooms Alley   Price\n",
            "0       NaN  Pave  127500\n",
            "1       2.0   NaN  106000\n",
            "2       4.0   NaN  178100\n",
            "3       NaN   NaN  140000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 6,
        "id": "WN4C5-3w_UyV",
        "colab_type": "text"
      },
      "source": [
        "## Handling Missing Data\n",
        "\n",
        "Note that \"NaN\" entries are missing values.\n",
        "To handle missing data, typical methods include *imputation* and *deletion*,\n",
        "where imputation replaces missing values with substituted ones,\n",
        "while deletion ignores missing values. Here we will consider imputation.\n",
        "\n",
        "By integer-location based indexing (`iloc`), we split `data` into `inputs` and `outputs`,\n",
        "where the former takes the first two columns while the latter only keeps the last column.\n",
        "For numerical values in `inputs` that are missing, we replace the \"NaN\" entries with the mean value of the same column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 7,
        "tab": [
          "mxnet"
        ],
        "id": "Coa-A_bG_UyX",
        "colab_type": "code",
        "colab": {},
        "outputId": "99f829e0-0391-4fbe-d785-1fefbd48a0b9"
      },
      "source": [
        "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
        "inputs = inputs.fillna(inputs.mean())\n",
        "print(inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   NumRooms Alley\n",
            "0       3.0  Pave\n",
            "1       2.0   NaN\n",
            "2       4.0   NaN\n",
            "3       3.0   NaN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "pzzQPsf3_Uyk",
        "colab_type": "text"
      },
      "source": [
        "For categorical or discrete values in `inputs`, we consider \"NaN\" as a category.\n",
        "Since the \"Alley\" column only takes two types of categorical values \"Pave\" and \"NaN\",\n",
        "`pandas` can automatically convert this column to two columns \"Alley_Pave\" and \"Alley_nan\".\n",
        "A row whose alley type is \"Pave\" will set values of \"Alley_Pave\" and \"Alley_nan\" to 1 and 0.\n",
        "A row with a missing alley type will set their values to 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 9,
        "tab": [
          "mxnet"
        ],
        "id": "ZVn1tW9H_Uym",
        "colab_type": "code",
        "colab": {},
        "outputId": "07fdbda9-b4f0-4654-a624-04046dccbea8"
      },
      "source": [
        "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
        "print(inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   NumRooms  Alley_Pave  Alley_nan\n",
            "0       3.0           1          0\n",
            "1       2.0           0          1\n",
            "2       4.0           0          1\n",
            "3       3.0           0          1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 10,
        "id": "Vt-mPt6G_Uy2",
        "colab_type": "text"
      },
      "source": [
        "## Conversion to the Tensor Format\n",
        "\n",
        "Now that all the entries in `inputs` and `outputs` are numerical, they can be converted to the tensor format.\n",
        "Once data are in this format, they can be further manipulated with those tensor functionalities that we have introduced in :numref:`sec_ndarray`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "mxnet"
        ],
        "id": "jGbnx09T_Uy3",
        "colab_type": "code",
        "colab": {},
        "outputId": "112763f7-5d1d-4096-9d1a-0769b970a21a"
      },
      "source": [
        "from mxnet import np\n",
        "\n",
        "X, y = np.array(inputs.values), np.array(outputs.values)\n",
        "X, y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[3., 1., 0.],\n",
              "        [2., 0., 1.],\n",
              "        [4., 0., 1.],\n",
              "        [3., 0., 1.]], dtype=float64),\n",
              " array([127500, 106000, 178100, 140000], dtype=int64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "F_L1EmLj_UzC",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Like many other extension packages in the vast ecosystem of Python, `pandas` can work together with tensors.\n",
        "* Imputation and deletion can be used to handle missing data.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "Create a raw dataset with more rows and columns.\n",
        "\n",
        "1. Delete the column with the most missing values.\n",
        "2. Convert the preprocessed dataset to the tensor format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 15,
        "tab": [
          "mxnet"
        ],
        "id": "qxn-lbZj_UzE",
        "colab_type": "text"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/28)\n"
      ]
    }
  ]
}