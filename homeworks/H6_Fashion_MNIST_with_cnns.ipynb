{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAKY12aeDDiO"
      },
      "source": [
        "**Chapter 14 – Deep Computer Vision Using Convolutional Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e03OPzUODDiT"
      },
      "source": [
        "_This notebook contains all the sample code in chapter 14._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JopOSX-mDDiY"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/14_deep_computer_vision_with_cnns.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1kUG6jmDDid"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUG13iqFDDih"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPzV31zfDDik",
        "outputId": "f50bade7-2bd7-472e-d815-3a7509f3d48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models, losses\n",
        "\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"cnn\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxx2-ZSDDi7"
      },
      "source": [
        "A couple utility functions to plot grayscale and RGB images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQchCPAoDDi-"
      },
      "source": [
        "def plot_image(image):\n",
        "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def plot_color_image(image):\n",
        "    plt.imshow(image, interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83mNjbcLDDoF"
      },
      "source": [
        "# Tackling Fashion MNIST With a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVgSda2FDDoG"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
        "\n",
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train = (X_train - X_mean) / X_std\n",
        "X_valid = (X_valid - X_mean) / X_std\n",
        "X_test = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_valid = X_valid[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numhiddens=2\n",
        "num_filters=32\n",
        "num_epochs=10\n",
        "\n",
        "DefaultConv2D = partial(keras.layers.Conv2D,\n",
        "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "model = tf.keras.Sequential()\n",
        "model.add(DefaultConv2D(filters=num_filters, kernel_size=5, input_shape=[28, 28, 1]))\n",
        "for i in range(numhiddens):\n",
        "    model.add(DefaultConv2D(filters=num_filters))\n",
        "    if(i%2==0):\n",
        "      num_filters=num_filters*2  \n",
        "      model.add(keras.layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb3F6Dis6ZTo",
        "outputId": "22d98cca-2547-4da1-b330-231df062b566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 28, 28, 32)        832       \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 14, 14, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 12544)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                802880    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 832,106\n",
            "Trainable params: 832,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IewxClAeDDoc",
        "outputId": "238e23b6-8bcf-4a44-b6f0-823898cf356c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
        "score = model.evaluate(X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 16s 7ms/step - loss: 0.7337 - accuracy: 0.7357 - val_loss: 0.4201 - val_accuracy: 0.8406\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4920 - accuracy: 0.8266 - val_loss: 0.3658 - val_accuracy: 0.8614\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4283 - accuracy: 0.8478 - val_loss: 0.3328 - val_accuracy: 0.8750\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3947 - accuracy: 0.8609 - val_loss: 0.3155 - val_accuracy: 0.8780\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3658 - accuracy: 0.8702 - val_loss: 0.2963 - val_accuracy: 0.8868\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3450 - accuracy: 0.8783 - val_loss: 0.2935 - val_accuracy: 0.8896\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3242 - accuracy: 0.8849 - val_loss: 0.2730 - val_accuracy: 0.8928\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3075 - accuracy: 0.8889 - val_loss: 0.2728 - val_accuracy: 0.8988\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2958 - accuracy: 0.8941 - val_loss: 0.2564 - val_accuracy: 0.9022\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2786 - accuracy: 0.9003 - val_loss: 0.2661 - val_accuracy: 0.9012\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2877 - accuracy: 0.8957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, compare how the model performs on the test dataset:"
      ],
      "metadata": {
        "id": "lI8Xh5BM4aO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjdMzBL44Zlg",
        "outputId": "b910ad45-82bf-4c04-8ca5-1d3007b90b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.2877 - accuracy: 0.8957 - 742ms/epoch - 2ms/step\n",
            "\n",
            "Test accuracy: 0.8956999778747559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction of a single image"
      ],
      "metadata": {
        "id": "u7uSfTpL4jDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_test[:10] # pretend we have new images\n",
        "y_pred = model.predict(X_new)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "AQGXdYvaDCuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5ecb65-d564-4318-a607-dba4788638a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 141ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.71345169e-09, 1.70923951e-08, 6.05659967e-09, 6.98474523e-09,\n",
              "        2.00400851e-09, 3.12830634e-05, 3.24559650e-08, 2.09485251e-03,\n",
              "        2.83858725e-07, 9.97873545e-01],\n",
              "       [3.55425691e-05, 3.97341576e-10, 9.98585939e-01, 4.57513011e-07,\n",
              "        7.84197531e-04, 1.65624930e-10, 5.93666453e-04, 3.12760061e-12,\n",
              "        8.46997850e-08, 1.29134490e-11],\n",
              "       [2.08135766e-08, 9.99999881e-01, 6.74223871e-11, 7.66416761e-08,\n",
              "        1.51976813e-08, 1.13624279e-12, 1.09951932e-08, 3.09670345e-12,\n",
              "        2.17374729e-12, 6.89775320e-12],\n",
              "       [2.95047098e-10, 9.99999642e-01, 4.89828447e-13, 3.63504910e-07,\n",
              "        6.58521460e-10, 5.89304809e-14, 6.65922803e-11, 8.63975368e-14,\n",
              "        1.30031470e-13, 5.72884839e-13],\n",
              "       [2.33706962e-02, 5.74157821e-06, 5.41824149e-03, 2.89824442e-03,\n",
              "        4.12345305e-03, 1.45684375e-04, 9.63959754e-01, 6.50849279e-06,\n",
              "        4.73884429e-05, 2.43310533e-05],\n",
              "       [9.70020778e-07, 9.99991298e-01, 4.37668817e-08, 5.53969039e-06,\n",
              "        1.17889681e-06, 4.45237136e-09, 1.00891407e-06, 8.63139893e-10,\n",
              "        3.96767241e-09, 5.20215382e-09],\n",
              "       [7.54870030e-07, 1.98424406e-08, 6.97308313e-03, 3.26059393e-07,\n",
              "        9.67851818e-01, 6.59488575e-10, 2.51739547e-02, 3.64156760e-11,\n",
              "        7.35464711e-09, 1.95285343e-10],\n",
              "       [9.03765067e-06, 1.07004347e-07, 2.70283286e-04, 1.11344734e-05,\n",
              "        1.33613255e-02, 4.71492285e-07, 9.86341774e-01, 2.41090081e-08,\n",
              "        5.85242788e-06, 5.73458081e-08],\n",
              "       [4.72183586e-07, 1.43561589e-08, 1.96031749e-07, 8.09542655e-07,\n",
              "        7.55418455e-07, 9.99305010e-01, 1.25717156e-06, 6.83566206e-04,\n",
              "        7.55241672e-06, 2.51027984e-07],\n",
              "       [1.24569285e-10, 2.20980775e-10, 1.25231089e-10, 9.22435461e-10,\n",
              "        2.79213763e-10, 1.14639835e-04, 1.13199294e-09, 9.99884605e-01,\n",
              "        1.57524287e-08, 6.55843394e-07]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FxVeyA6DDs5"
      },
      "source": [
        "#Homework 5: High Accuracy CNN for Fashion MNIST\n",
        "_Exercise: Build your own CNN from scratch and try to achieve the highest possible accuracy on Fashion MNIST._"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use can consider different architectures, BatchNormalization, LayerNormalization, Dropout, SkipConnections, data augmentation techniques, etc.\n",
        "\n",
        "Moreover you can consider to use Grid search or Randomized search or [Optuna](https://colab.research.google.com/github/serivan/mldmlab/blob/master/Solutions/Optuna_bayesian_hyperparameter_tuning.ipynb) in order to  define the hyperparameters."
      ],
      "metadata": {
        "id": "GMwG3LMnH0n6"
      }
    }
  ]
}